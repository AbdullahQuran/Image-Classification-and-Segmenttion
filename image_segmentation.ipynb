{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_Segmentation(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2P2q4wTPBho",
        "colab_type": "text"
      },
      "source": [
        "Solution to homework 2: using UNET model for sematic segmentation \n",
        "\n",
        "Val_accuracy:  - test accuracy: \n",
        "\n",
        "parameters:\n",
        "\n",
        "4 down sampling layers\n",
        "4 up samling layers  \n",
        "1 sigmiod layer  \n",
        "\n",
        "optimizer: ADAM with lr = 0.0023\n",
        "dropout: 50% \n",
        "Augmentation: no\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCHk9aZoHjRE",
        "colab_type": "text"
      },
      "source": [
        "Impoting data from googe drive and importing the useful libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMfjv9JgBJku",
        "colab_type": "code",
        "outputId": "6acfd206-1d2d-47e7-b900-8299d6a17851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',  force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvqjdfqPJOcr",
        "colab_type": "code",
        "outputId": "0a9bea1f-4524-44be-eac8-73d9d6ec3f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#try:\n",
        "  # Use the %tensorflow_version magic if in colab.\n",
        "  #%tensorflow_version 2.x\n",
        "#except Exception:\n",
        "  #pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQdl9VL4_dFl",
        "colab_type": "code",
        "outputId": "021c09d9-af1c-4395-956a-fe158f68a948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import shutil\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras import backend as K\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.losses import binary_crossentropy\n",
        "from tensorflow.python.keras import losses\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense,GlobalAveragePooling2D,Dropout,SeparableConv2D, ZeroPadding2D,  Conv2DTranspose, BatchNormalization, Activation, Add, Conv2D, Flatten, LeakyReLU, MaxPooling2D, UpSampling2D, concatenate\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.datasets import load_files  # to load the images from the subfolders    \n",
        "from keras.utils import np_utils # to hot encode the label of the image\n",
        "import matplotlib.pyplot as plt # to draw barplot and etc...\n",
        "from tqdm import tqdm \n",
        "from keras.preprocessing import image    \n",
        "from keras.callbacks import ModelCheckpoint # to save the best weights for the model while training \n",
        "import random as rn\n",
        "from sklearn.utils import class_weight\n",
        "from datetime import datetime\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JrLR1yrpoO4",
        "colab_type": "text"
      },
      "source": [
        "Fixing a random seed to make the results reproducible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd2t5q0oZ3-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rand_seed = 1234\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "#random seed for NP genreator of ranodm numbers\n",
        "np.random.seed(rand_seed)\n",
        "\n",
        "#random seed generator for Python\n",
        "rn.seed(rand_seed)\n",
        "\n",
        "#random seed for tensorflow\n",
        "tf.set_random_seed(rand_seed)\n",
        "\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag5tO_LepxNc",
        "colab_type": "text"
      },
      "source": [
        " since the data is not splitted into training and vailation sets. i will do so in the next code .The code below creates a train and a val folder each containing 20 folders (one for each type of class). It then moves the images from the original folders to these new folders such that 80% of the images go to the training set and 20% of the images go into the validation set. In the end our directory will have the \n",
        "\n",
        "Since I did nott delete the original folders, they will still be in the directory which is ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBuXY_0F92LF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_list = []\n",
        "val_list = []\n",
        "train_dict = {}\n",
        "val_dict = {}\n",
        "############################################################### Generating JSON file for data Split ##########################################\n",
        "#for image in glob.glob(os.path.join(base_dir, 'train', 'images') + '/*.jpg'):\n",
        "    #  train_list.append(image)\n",
        "   #   train_dict.setdefault (images , []).append(os.path.basename(image)) \n",
        "      #print(image)\n",
        "      \n",
        "#for image in glob.glob(os.path.join(base_dir, 'val', 'images') + '/*.jpg'):\n",
        " #     val_list.append(image)\n",
        "  #    val_dict.setdefault (images , []).append(os.path.basename(image))   \n",
        "   \n",
        "#dataset_split = {'training' : train_dict , 'validation' : val_dict }\n",
        "\n",
        "#print(dataset_split)\n",
        "#import json\n",
        "#with open('dataset_split.json', 'w') as fp:\n",
        "#      json.dump(dataset_split, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyE5IIBQDs8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gRM0uV_DtQO",
        "colab_type": "text"
      },
      "source": [
        "preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-Bf0AY8aP4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_dir = '/content/drive/My Drive/Segmentation_Dataset_/training'\n",
        "\n",
        "train_dir = os.path.join(b_dir, 'train')\n",
        "val_dir = os.path.join(b_dir, 'val')\n",
        "\n",
        "train_image_dir = os.path.join(train_dir, 'image')\n",
        "train_mask_dir = os.path.join(train_dir, 'mask')\n",
        "\n",
        "val_image_dir = os.path.join(val_dir, 'image')\n",
        "val_mask_dir = os.path.join(val_dir, 'mask')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tomJ2d6odmDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_gen_train = ImageDataGenerator(rescale=1./255)\n",
        "mask_gen_train = ImageDataGenerator(rescale=1./255)\n",
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "mask_gen_val = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zIsiAmQC7ct",
        "colab_type": "code",
        "outputId": "672cba5a-44be-4470-a87d-51039781742b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "batch_size = 128\n",
        "IMG_SHAPE = 256\n",
        "\n",
        "train_img_gen = image_gen_train.flow_from_directory(\n",
        "                                                directory= train_image_dir,\n",
        "                                                shuffle=True,\n",
        "                                                batch_size=8,\n",
        "                                                target_size=(IMG_SHAPE,IMG_SHAPE),\n",
        "                                                class_mode=None,\n",
        "                                                interpolation='bilinear',\n",
        "                                                seed = rand_seed                                                \n",
        "                                                )\n",
        "\n",
        "\n",
        "train_mask_gen = mask_gen_train.flow_from_directory(\n",
        "                                                batch_size=8,\n",
        "                                                directory= train_mask_dir,\n",
        "                                                shuffle=True,\n",
        "                                                target_size=(IMG_SHAPE,IMG_SHAPE),\n",
        "                                                class_mode=None,\n",
        "                                                interpolation='bilinear',\n",
        "                                                seed = rand_seed                                                \n",
        "                                                )\n",
        "\n",
        "train_gen = zip(train_img_gen, train_mask_gen)\n",
        "\n",
        "\n",
        "\n",
        "val_img_gen = image_gen_val.flow_from_directory(\n",
        "                                                 batch_size=4,\n",
        "                                                 directory=val_image_dir,\n",
        "                                                 shuffle=True,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode=None,\n",
        "                                                 interpolation='bilinear',\n",
        "                                                 seed = rand_seed\n",
        "                                                 )\n",
        "\n",
        "val_mask_gen = mask_gen_val.flow_from_directory(\n",
        "                                                 batch_size=4,\n",
        "                                                 directory=val_mask_dir,\n",
        "                                                 shuffle=True,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode=None,\n",
        "                                                 interpolation='bilinear',\n",
        "                                                 seed = rand_seed\n",
        "                                                 )\n",
        "\n",
        "valid_gen = zip(val_img_gen, val_mask_gen)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6116 images belonging to 1 classes.\n",
            "Found 6116 images belonging to 1 classes.\n",
            "Found 1531 images belonging to 1 classes.\n",
            "Found 1531 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb_D9XrP6IG_",
        "colab_type": "code",
        "outputId": "32d68f28-2c25-4c76-ed05-cb3e9d00ad3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, IMG_SHAPE, IMG_SHAPE, 3], [None, IMG_SHAPE, IMG_SHAPE, 3]))\n",
        "def prepare_target(x_, y_):\n",
        "    y_ = tf.cast(tf.expand_dims(y_[..., 0], -1), tf.int32)\n",
        "    print (y_)\n",
        "    return x_, y_\n",
        "\n",
        "train_dataset = train_dataset.map(prepare_target)\n",
        "print (train_dataset)\n",
        "\n",
        "# Repeat\n",
        "train_dataset = train_dataset.repeat()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Cast:0\", shape=(?, 256, 256, 1), dtype=int32)\n",
            "<DatasetV1Adapter shapes: ((?, 256, 256, 3), (?, 256, 256, 1)), types: (tf.float32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWP9kPjz7yxL",
        "colab_type": "code",
        "outputId": "0016ba9b-777e-4808-8fea-dd720425cacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Validation\n",
        "# ----------\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, IMG_SHAPE, IMG_SHAPE, 3], [None, IMG_SHAPE, IMG_SHAPE, 3]))\n",
        "valid_dataset = valid_dataset.map(prepare_target)\n",
        "\n",
        "# Repeat\n",
        "valid_dataset = valid_dataset.repeat()\n",
        "print (valid_dataset)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Cast:0\", shape=(?, 256, 256, 1), dtype=int32)\n",
            "<DatasetV1Adapter shapes: ((?, 256, 256, 3), (?, 256, 256, 1)), types: (tf.float32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLqdQQIkCfRP",
        "colab_type": "text"
      },
      "source": [
        "defining the Loss function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON42S9RbaxrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_IoU(y_true, y_pred):\n",
        "    # from pobability to predicted class {0, 1}\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32) # when using sigmoid. Use argmax for softmax\n",
        "    # A and B\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    # A or B\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    # IoU\n",
        "    return intersection / union"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbxjLmFJ2uiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    # Flatten\n",
        "    y_pred = tf.cast(y_pred > 0.5 , tf.float32) # when using sigmoid. Use argmax for softmax\n",
        "    y_true = tf.cast(y_true, tf.float32) # when using sigmoid. Use argmax for softmax\n",
        "\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPJBXgokMLxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIXt8thNMORX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bce_dice_loss(y_true, y_pred):\n",
        "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pUUmAEGnNNw",
        "colab_type": "text"
      },
      "source": [
        "building UNET model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwO5Qr8L5DE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    inputs = tf.keras.layers.Input((IMG_SHAPE,IMG_SHAPE,3))\n",
        "    conv1 = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',)(inputs)\n",
        "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "    conv2 = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    \n",
        "    conv3 = tf.keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = tf.keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "   \n",
        "    conv4 = tf.keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = tf.keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = tf.keras.layers.Dropout(0.5)(conv4)\n",
        "    pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = tf.keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = tf.keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = tf.keras.layers.Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = tf.keras.layers.Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = tf.keras.layers.concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = tf.keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = tf.keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = tf.keras.layers.Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = tf.keras.layers.concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = tf.keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = tf.keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = tf.keras.layers.Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = tf.keras.layers.concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = tf.keras.layers.Conv2D(16, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = tf.keras.layers.concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = tf.keras.layers.Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = tf.keras.layers.Conv2D(1, (1,1), activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = tf.keras.Model(inputs = [inputs], outputs = [conv10])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2BpemApOruZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kVkBlN1wJZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiyz3dWiDIpv",
        "colab_type": "text"
      },
      "source": [
        "Compiling and training the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2adANaipHPXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.0023\n",
        "decay_rate = learning_rate / epochs\n",
        "\n",
        "opt1 = tf.keras.optimizers.SGD(lr=learning_rate,\n",
        "            momentum = .9, \n",
        "            decay = decay_rate,\n",
        "            nesterov=True\n",
        "           )\n",
        "\n",
        "opt2 = adam(lr=learning_rate, \n",
        "            beta_1=0.9, beta_2=0.999, epsilon=None, \n",
        "            decay=decay_rate, amsgrad=False)\n",
        "\n",
        "model.compile(optimizer = opt2,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[dice_loss])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkK7EEasLjoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpLh79aU8YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [my_IoU])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0vLcWZ-ZR-3",
        "colab_type": "code",
        "outputId": "f6577fc1-85f2-43cf-831c-43572738f38d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='/content/drive/My Drive/weights.best.segmentation_acc.hdf5', \n",
        "                               monitor='val_loss',\n",
        "                               verbose=1, \n",
        "                               save_best_only=True\n",
        "                               )\n",
        "                               \n",
        "history = model.fit(train_dataset,\n",
        "          epochs=100,  #### set repeat in training dataset\n",
        "          steps_per_epoch=5,\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=5,\n",
        "          callbacks= [checkpointer])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 5 steps, validate for 5 steps\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "4/5 [=======================>......] - ETA: 2s - loss: 1.2017 - dice_loss: 0.5513\n",
            "Epoch 00001: val_loss improved from inf to 1.22551, saving model to /content/drive/My Drive/weights.best.segmentation_acc.hdf5\n",
            "5/5 [==============================] - 14s 3s/step - loss: 1.1799 - dice_loss: 0.5478 - val_loss: 1.2255 - val_dice_loss: 0.5262\n",
            "Epoch 2/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.0774 - dice_loss: 0.5212\n",
            "Epoch 00002: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 250ms/step - loss: 1.0945 - dice_loss: 0.5398 - val_loss: 1.2400 - val_dice_loss: 0.5573\n",
            "Epoch 3/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.1397 - dice_loss: 0.5684\n",
            "Epoch 00003: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 1.1003 - dice_loss: 0.5483 - val_loss: 1.3884 - val_dice_loss: 0.7266\n",
            "Epoch 4/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.9460 - dice_loss: 0.4282\n",
            "Epoch 00004: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.9487 - dice_loss: 0.4356 - val_loss: 1.4651 - val_dice_loss: 0.7868\n",
            "Epoch 5/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.0272 - dice_loss: 0.4930\n",
            "Epoch 00005: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 234ms/step - loss: 0.9882 - dice_loss: 0.4731 - val_loss: 4.8954 - val_dice_loss: 0.4882\n",
            "Epoch 6/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.0218 - dice_loss: 0.5085\n",
            "Epoch 00006: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 247ms/step - loss: 1.0452 - dice_loss: 0.5043 - val_loss: 1.5351 - val_dice_loss: 0.5434\n",
            "Epoch 7/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.8617 - dice_loss: 0.3987\n",
            "Epoch 00007: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 249ms/step - loss: 0.8509 - dice_loss: 0.3986 - val_loss: 1.3230 - val_dice_loss: 0.6785\n",
            "Epoch 8/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7718 - dice_loss: 0.3411\n",
            "Epoch 00008: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 0.7899 - dice_loss: 0.3421 - val_loss: 1.6445 - val_dice_loss: 1.0000\n",
            "Epoch 9/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.9615 - dice_loss: 0.4430\n",
            "Epoch 00009: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 267ms/step - loss: 0.9224 - dice_loss: 0.4253 - val_loss: 1.6499 - val_dice_loss: 0.9999\n",
            "Epoch 10/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7781 - dice_loss: 0.3577\n",
            "Epoch 00010: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 266ms/step - loss: 0.8023 - dice_loss: 0.3649 - val_loss: 1.6488 - val_dice_loss: 1.0000\n",
            "Epoch 11/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.8280 - dice_loss: 0.3974\n",
            "Epoch 00011: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 260ms/step - loss: 0.9011 - dice_loss: 0.4265 - val_loss: 25.1517 - val_dice_loss: 0.5860\n",
            "Epoch 12/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.0538 - dice_loss: 0.5318\n",
            "Epoch 00012: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 268ms/step - loss: 1.0406 - dice_loss: 0.5145 - val_loss: 89.4800 - val_dice_loss: 0.4770\n",
            "Epoch 13/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.8823 - dice_loss: 0.4383\n",
            "Epoch 00013: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 261ms/step - loss: 0.8851 - dice_loss: 0.4309 - val_loss: 17.9920 - val_dice_loss: 0.5295\n",
            "Epoch 14/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.9082 - dice_loss: 0.4256\n",
            "Epoch 00014: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 249ms/step - loss: 0.9554 - dice_loss: 0.4514 - val_loss: 1.5393 - val_dice_loss: 0.5995\n",
            "Epoch 15/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7887 - dice_loss: 0.3429\n",
            "Epoch 00015: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 247ms/step - loss: 0.8027 - dice_loss: 0.3534 - val_loss: 1.6589 - val_dice_loss: 1.0000\n",
            "Epoch 16/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7862 - dice_loss: 0.3528\n",
            "Epoch 00016: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.7820 - dice_loss: 0.3505 - val_loss: 1.6212 - val_dice_loss: 1.0000\n",
            "Epoch 17/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7394 - dice_loss: 0.3267\n",
            "Epoch 00017: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.7318 - dice_loss: 0.3236 - val_loss: 1.5883 - val_dice_loss: 1.0000\n",
            "Epoch 18/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7388 - dice_loss: 0.3363\n",
            "Epoch 00018: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.7526 - dice_loss: 0.3402 - val_loss: 1.6267 - val_dice_loss: 1.0000\n",
            "Epoch 19/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7542 - dice_loss: 0.3198\n",
            "Epoch 00019: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.7627 - dice_loss: 0.3287 - val_loss: 1.6174 - val_dice_loss: 1.0000\n",
            "Epoch 20/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6899 - dice_loss: 0.3019\n",
            "Epoch 00020: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 0.7092 - dice_loss: 0.3165 - val_loss: 1.6238 - val_dice_loss: 1.0000\n",
            "Epoch 21/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7751 - dice_loss: 0.3379\n",
            "Epoch 00021: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 247ms/step - loss: 0.7863 - dice_loss: 0.3368 - val_loss: 1.5707 - val_dice_loss: 0.9394\n",
            "Epoch 22/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7082 - dice_loss: 0.3069\n",
            "Epoch 00022: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 250ms/step - loss: 0.7161 - dice_loss: 0.3130 - val_loss: 1.3432 - val_dice_loss: 0.8026\n",
            "Epoch 23/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7245 - dice_loss: 0.3275\n",
            "Epoch 00023: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 0.7117 - dice_loss: 0.3223 - val_loss: 1.5708 - val_dice_loss: 1.0000\n",
            "Epoch 24/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.8441 - dice_loss: 0.3748\n",
            "Epoch 00024: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.8550 - dice_loss: 0.3747 - val_loss: 1.6327 - val_dice_loss: 1.0000\n",
            "Epoch 25/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7191 - dice_loss: 0.3339\n",
            "Epoch 00025: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 247ms/step - loss: 0.7372 - dice_loss: 0.3378 - val_loss: 1.5820 - val_dice_loss: 1.0000\n",
            "Epoch 26/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.8882 - dice_loss: 0.3885\n",
            "Epoch 00026: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 246ms/step - loss: 0.8534 - dice_loss: 0.3775 - val_loss: 1.6627 - val_dice_loss: 1.0000\n",
            "Epoch 27/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7003 - dice_loss: 0.3124\n",
            "Epoch 00027: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 247ms/step - loss: 0.6913 - dice_loss: 0.3065 - val_loss: 1.7567 - val_dice_loss: 0.9943\n",
            "Epoch 28/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7280 - dice_loss: 0.3138\n",
            "Epoch 00028: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 251ms/step - loss: 0.7339 - dice_loss: 0.3195 - val_loss: 1.6286 - val_dice_loss: 1.0000\n",
            "Epoch 29/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7047 - dice_loss: 0.3004\n",
            "Epoch 00029: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.7192 - dice_loss: 0.3081 - val_loss: 1.6712 - val_dice_loss: 1.0000\n",
            "Epoch 30/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7902 - dice_loss: 0.3478\n",
            "Epoch 00030: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.7470 - dice_loss: 0.3315 - val_loss: 1.6657 - val_dice_loss: 1.0000\n",
            "Epoch 31/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6814 - dice_loss: 0.2988\n",
            "Epoch 00031: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 244ms/step - loss: 0.6770 - dice_loss: 0.2938 - val_loss: 1.6236 - val_dice_loss: 1.0000\n",
            "Epoch 32/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7288 - dice_loss: 0.3222\n",
            "Epoch 00032: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 0.7074 - dice_loss: 0.3132 - val_loss: 1.7167 - val_dice_loss: 1.0000\n",
            "Epoch 33/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7614 - dice_loss: 0.3138\n",
            "Epoch 00033: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 236ms/step - loss: 0.7512 - dice_loss: 0.3121 - val_loss: 1.6566 - val_dice_loss: 1.0000\n",
            "Epoch 34/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7597 - dice_loss: 0.3352\n",
            "Epoch 00034: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.8044 - dice_loss: 0.3552 - val_loss: 1.6839 - val_dice_loss: 1.0000\n",
            "Epoch 35/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7238 - dice_loss: 0.2964\n",
            "Epoch 00035: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.7242 - dice_loss: 0.3036 - val_loss: 1.9215 - val_dice_loss: 1.0000\n",
            "Epoch 36/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7952 - dice_loss: 0.3348\n",
            "Epoch 00036: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 261ms/step - loss: 0.7833 - dice_loss: 0.3348 - val_loss: 1.7579 - val_dice_loss: 1.0000\n",
            "Epoch 37/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6790 - dice_loss: 0.2891\n",
            "Epoch 00037: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 242ms/step - loss: 0.6806 - dice_loss: 0.2929 - val_loss: 1.8162 - val_dice_loss: 1.0000\n",
            "Epoch 38/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.8456 - dice_loss: 0.3740\n",
            "Epoch 00038: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 0.8661 - dice_loss: 0.3964 - val_loss: 1.8254 - val_dice_loss: 1.0000\n",
            "Epoch 39/100\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7446 - dice_loss: 0.3330\n",
            "Epoch 00039: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.7653 - dice_loss: 0.3345 - val_loss: 2.0346 - val_dice_loss: 1.0000\n",
            "Epoch 40/100\n",
            "4/5 [=======================>......] - ETA: 1s - loss: 0.6993 - dice_loss: 0.3128\n",
            "Epoch 00040: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 21s 4s/step - loss: 0.7306 - dice_loss: 0.3130 - val_loss: 1.8012 - val_dice_loss: 1.0000\n",
            "Epoch 41/100\n",
            "4/5 [=======================>......] - ETA: 4s - loss: 0.6704 - dice_loss: 0.2925\n",
            "Epoch 00041: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 36s 7s/step - loss: 0.6930 - dice_loss: 0.3057 - val_loss: 1.9745 - val_dice_loss: 1.0000\n",
            "Epoch 42/100\n",
            "4/5 [=======================>......] - ETA: 5s - loss: 0.8007 - dice_loss: 0.3543\n",
            "Epoch 00042: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 36s 7s/step - loss: 0.7767 - dice_loss: 0.3375 - val_loss: 1.8596 - val_dice_loss: 1.0000\n",
            "Epoch 43/100\n",
            "4/5 [=======================>......] - ETA: 5s - loss: 0.7576 - dice_loss: 0.2991 \n",
            "Epoch 00043: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 38s 8s/step - loss: 0.7081 - dice_loss: 0.2777 - val_loss: 1.8908 - val_dice_loss: 1.0000\n",
            "Epoch 44/100\n",
            "4/5 [=======================>......] - ETA: 4s - loss: 0.7231 - dice_loss: 0.3147\n",
            "Epoch 00044: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 36s 7s/step - loss: 0.7463 - dice_loss: 0.3247 - val_loss: 1.8691 - val_dice_loss: 1.0000\n",
            "Epoch 45/100\n",
            "4/5 [=======================>......] - ETA: 5s - loss: 0.7038 - dice_loss: 0.3032 \n",
            "Epoch 00045: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 37s 7s/step - loss: 0.7092 - dice_loss: 0.3072 - val_loss: 1.8077 - val_dice_loss: 1.0000\n",
            "Epoch 46/100\n",
            "4/5 [=======================>......] - ETA: 4s - loss: 0.7000 - dice_loss: 0.3043\n",
            "Epoch 00046: val_loss did not improve from 1.22551\n",
            "5/5 [==============================] - 35s 7s/step - loss: 0.7346 - dice_loss: 0.3193 - val_loss: 1.7721 - val_dice_loss: 1.0000\n",
            "Epoch 47/100\n",
            "3/5 [=================>............] - ETA: 10s - loss: 0.7507 - dice_loss: 0.3575"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5779a56e62e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m           callbacks= [checkpointer])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lFW8dfBsnI0",
        "colab_type": "text"
      },
      "source": [
        "The remaining cells do proccesing to convert images to 4D, to predicat on the test data and to generate CSV submission for Kaggle. at the end the predicted image is plotted along with the respective model prediction  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxyro-EwYLAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/drive/My Drive/weights.best.segmentation_acc.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwfHlFpEYYTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = load_files(\"/content/drive/My Drive/Segmentation_Dataset_/test/images/img\") #this line takes too much time on colab to execute 5 min "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YJzoePF2wo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_img(img_path):\n",
        "    img=path_to_tensor(img_path).astype('float32')/255\n",
        "    # obtain predicted vector\n",
        "    predicted_vector = model.predict(img)\n",
        "    return predicted_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2DEUfPE2sWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def path_to_tensor(img_path):\n",
        "    # loads RGB image as PIL.Image.Image type\n",
        "    img = image.load_img(img_path, target_size=(256, 256))\n",
        "    # convert PIL.Image.Image type to 3D tensor with shape (256, 256, 3)\n",
        "    x = image.img_to_array(img)\n",
        "    # convert 3D tensor to 4D tensor with shape (1, 256, 256, 3) and return 4D tensor\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "def paths_to_tensor(img_paths):\n",
        "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "    return np.vstack(list_of_tensors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI532nfiD9-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rle_encode(img):\n",
        "      # Flatten column-wise\n",
        "      pixels = img.T.flatten()\n",
        "      pixels = np.concatenate([[0], pixels, [0]])\n",
        "      runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "      runs[1::2] -= runs[::2]\n",
        "      return ' '.join(str(x) for x in runs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da-EgfonDjJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(csv_fname, 'w') as f:\n",
        "\n",
        "      f.write('ImageId,EncodedPixels,Width,Height\\n')\n",
        "\n",
        "      for key, value in results.items():\n",
        "          f.write(key + ',' + str(value) + ',' + '256' + ',' + '256' + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GalDBNiPYdbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_list = []\n",
        "results = {}\n",
        "\n",
        "for filename in glob.glob('/content/drive/My Drive/Segmentation_Dataset_/test/images/img/*.tif'): \n",
        "    image_list.append(filename)\n",
        "\n",
        "for image_name in image_list:\n",
        "    results[os.path.splitext(os.path.basename(image_name))[0]] = rle_encode(np.round((predict_img(image_name))))\n",
        "\n",
        "\n",
        "create_csv(results)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}